{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'text.txt'}, page_content='Food: A Universal Language\\nFood is an essential part of our lives, not only as a source of nutrition but also as a way to connect with others and explore different cultures. Around the world, each country has its unique cuisine that tells a story of its history, geography, and traditions.\\n\\nOne of the most universally loved foods is bread. From French baguettes to Indian naan and Mexican tortillas, bread comes in countless forms and flavors. ItвЂ™s a staple that reflects the creativity of people using simple ingredients like flour, water, and salt.\\n\\nFruits and vegetables are another key part of many diets, offering a rainbow of colors and flavors. For instance, tropical fruits like mangoes and pineapples are sweet and juicy, while root vegetables like carrots and potatoes are hearty and versatile.\\n\\nMeals often revolve around proteins, whether itвЂ™s grilled chicken, fresh fish, or plant-based options like tofu and lentils. Protein is vital for energy and strength, and cultures worldwide have found innovative ways to prepare it. For example, Japanese sushi showcases raw fish, while Italian cuisine is famous for dishes like spaghetti carbonara with eggs and pancetta.\\n\\nFood is also about indulgence and enjoyment. Desserts like cakes, pastries, and ice cream bring joy to people of all ages. Every culture has its sweet traditions, such as baklava in the Middle East or tiramisu in Italy.\\n\\nFinally, food brings people together. Sharing a meal with family and friends is one of lifeвЂ™s greatest pleasures. Festivals, celebrations, and gatherings often center around food, making it a universal language that everyone understands.\\n\\nIn the end, food is more than just sustenance; itвЂ™s a way to experience the world, one bite at a time.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"text.txt\")\n",
    "text_doc = loader.load()\n",
    "text_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://python.langchain.com/docs/how_to/document_loader_web/\",),\n",
    "                        bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                          class_=(\"language-python codeBlockContainer_Ckt0 theme-code-block\", \n",
    "                                  \"language-output codeBlockContainer_Ckt0 theme-code-block\", \n",
    "                                  \"anchor anchorWithStickyNavbar_LWe7\") \n",
    "                        )))\n",
    "text_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/'}, page_content='Setup\\u200b%pip install -qU langchain-community beautifulsoup4%pip install -qU langchain-unstructuredSimple and fast text extraction\\u200bimport bs4from langchain_community.document_loaders import WebBaseLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = WebBaseLoader(web_paths=[page_url])docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]USER_AGENT environment variable not set, consider setting it to identify your requests.print(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500].strip()){\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\', \\'title\\': \\'How to add memory to chatbots | \\\\uf8ffü¶úÔ∏è\\\\uf8ffüîó LangChain\\', \\'description\\': \\'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\\', \\'language\\': \\'en\\'}How to add memory to chatbots | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChainSkip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdloader = WebBaseLoader(    web_paths=[page_url],    bs_kwargs={        \"parse_only\": bs4.SoupStrainer(class_=\"theme-doc-markdown markdown\"),    },    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]print(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500]){\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\'}How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We\\'ll go into more detail on a few techniqprint(doc.page_content[-500:])a greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.\\'), | HumanMessage(content=\\'What did I say my name was?\\'), | AIMessage(content=\\'You introduced yourself as Nemo. How can I assist you today, Nemo?\\')] | Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.Advanced parsing\\u200bfrom langchain_unstructured import UnstructuredLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = UnstructuredLoader(web_url=page_url)docs = []async for doc in loader.alazy_load():    docs.append(doc)INFO: Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.INFO: NumExpr defaulting to 8 threads.for doc in docs[:5]:    print(doc.page_content)How to add memory to chatbotsA key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:Simply stuffing previous messages into a chat model prompt.The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.More complex modifications like synthesizing summaries for long running conversations.ERROR! Session/line number was not unique in database. History logging moved to new session 2747Extracting content from specific sections\\u200bfor doc in docs[:5]:    print(f\\'{doc.metadata[\"category\"]}: {doc.page_content}\\')Title: How to add memory to chatbotsNarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:ListItem: Simply stuffing previous messages into a chat model prompt.ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.ListItem: More complex modifications like synthesizing summaries for long running conversations.from typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)from collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text){\\'https://python.langchain.com/docs/how_to/chatbots_memory/\\': \"You\\'ll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\\\\n%pip install --upgrade --quiet langchain langchain-openai\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\\\\nYou should consider upgrading via the \\'/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip\\' command.[0m[33m\\\\n[0mNote: you may need to restart the kernel to use updated packages.\\\\n\", \\'https://python.langchain.com/docs/how_to/chatbots_tools/\\': \"For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\\\nYou\\'ll need to sign up for an account on the Tavily website, and install the following packages:\\\\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\\\\n\"}Vector search over page content\\u200b%pip install -qU langchain-openaiimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")from langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())retrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)for doc in retrieved_docs:    print(f\\'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\\\n\\')INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"``````outputPage https://python.langchain.com/docs/how_to/chatbots_tools/: You\\'ll need to sign up for an account on the Tavily website, and install the following packages:Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.Other web page loaders\\u200b')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Yevheniia Ilchenko. Python_AI_ML.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Yevheniia Ilchenko. Python_AI_ML.pdf', 'page': 0}, page_content='Yevheniia Ilchenko\\nPYTHON ENGINEER (AI | ML)\\nilchenko.evheniia@gmail.com\\n \\n+38 050 60 51 644\\n \\nUkraine/ Kyiv\\n \\nLinkedIn\\n \\nGithub\\n \\nTelegram\\n \\nReady to work autonomously in case of blackout\\n \\nProfile\\nPython engineer with analytical knowledge, strategic \\nplanning, and strong organizational skills, bringing with it a \\nunique combination of leadership and technical skills.  I have  \\nknowledge of  AI/ML, worked with LLM, SD and have basic \\nskills in AWS/GCP. \\nI specialize in Stable Diffusion, and have experience in web \\napplication development using Django/DRF/FastAPI. \\nProficient with tools like Git/GitHub/GitLab, Docker.\\nI have more than 5 years of sales management experience, so \\nI quickly understand the problems of business to effectively \\nsolve them.\\nSkills\\nPython 3.7 + & OOP\\nStable Diffusion — gen pictures and videos + Pillow, Moviepy\\nLLM — LangChain, HuggingFace | Streamlit for UI\\nData Analysis — NumPy, Pandas, Matplotlib\\nMachine Learning — regression and classification algorithms\\nTools — Git/Github/Gitlab, Docker, Jenkins\\nWeb Frameworks — Django ORM, Django, Django Rest  \\n Flask/FastAPI(basic)\\nDatabases & SQL — PostgreSQL, SQLite\\nLinux Administration\\nAWS — EC2, EBS/EFS, S3, VPC, RDS\\nWeb Scraping — Selenium, Scrapy (basic)\\nProject management/ Time management/ Market Analysis\\n— Trello, Jira, CRM  Miro, Roadmap, GanttPro chart, Canvas\\nEnglish — Upper-Intermediate\\nProfessional Experience\\nPython Engineer, Reface\\n08.2024 – present\\nTechnologies: Python, Stable Diffusion, Streamlit, ML|AI, Numpy, \\nPandas, Docker, GitLab, GCP, Linux, Pillow, Moviepy\\nResponsibilities:\\n•Integration of Stable Diffusion into projects\\n•Setting up and testing the generation of pictures/videos of the SD\\n•Fix problems/bugs\\n•Packaging in UI format (Streamlit)\\n•Support motion designers(GenAI)\\nPython Developer, Freelancer\\n01.2024 – 07.2024\\nTechnologies: Python, SQL, Django, DRF, Streamlit, ML, \\nPostgreSQL, Docker, LLM, AWS\\nResponsibilities:\\n•Building applications using Django, DRF, router, class, and \\nfunctional components;\\n•Handling code reviews, investigating, reproducing, and fixing \\nbugs.\\n•Integration LLM and create chat-bot with AI assistent\\n•Building dash-boards for analysis date\\nProjects\\n🔍 AppVideoChange\\n07.2024 – 07.2024\\nThis project is a web application for manipulating video files with \\nuse LLM. Neural network Riffusion creates a visual representation of \\nthe sound (spectogram), and then converts it into audio. It allows \\nusers to upload a video, split it into multiple clips, generate an audio \\ntrack based on a user-provided prompt, and replace the audio of a \\nspecified clip with the generated audio. \\nSkills: LLM, Streamlit, MoviePy, Riffusion, pydub\\n🔍 Theatre service API\\nDjango-REST project for theatre clients. Using this API, they can \\nreservation tickets for performances. This API has endpoints that are \\ndocumented using swagger.\\nSkills: Object-Oriented Programming (OOP) · REST APIs · Docker · \\nPostgreSQL · Swagger API · Django REST Framework · Python \\n(Programming Language)\\nOther Professional Experience\\nRegional Manager and Analyst of the Center-Sourth-East Region\\n04.2017 – 10.2023\\nResponsibilities:\\n•Successful management and training of a team of 15 people.\\n•Market analysis and development of congregational strategies, \\nwith the result that sales were up to an average of 20% per year;\\n•Successful negotiations in networks and conclusion of MD, and as \\na result, an increase and retention of a share of more than 50% of \\nthe entire Ukrainian market for medicinal herbs;\\n•Organization and holding of medical conferences more than 20 \\nin the last 4 years;\\n•Personal growth from a medical representative to a regency \\nmanager responsible for 2/3 of the territory of Ukraine.\\nCourses\\nPython course, Mate academy 09.2023 – 05.2024\\nUltimate AWS Certified Solutions Architect Associate SAA-C03, Udemy 06.2024 – 06.2024\\nLinux Administration: The Complete Linux Bootcamp for 2024, Udemy 07.2024 – 08.2024\\nChatGPT Complete Guide: Learn Midjourney, ChatGPT 4 & More, Udemy 06.2024 – 06.2024\\nMathematical Foundations of Machine Learning, Udemy 08.2024 – 08.2024\\nMachine Learning from 0 to Hero, Data Loves Academy 11.2024 – present\\n|\\n|')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Yevheniia Ilchenko. Python_AI_ML.pdf', 'page': 0}, page_content='Yevheniia Ilchenko\\nPYTHON ENGINEER (AI | ML)\\nilchenko.evheniia@gmail.com\\n \\n+38 050 60 51 644'),\n",
       " Document(metadata={'source': 'Yevheniia Ilchenko. Python_AI_ML.pdf', 'page': 0}, page_content='+38 050 60 51 644\\n \\nUkraine/ Kyiv\\n \\nLinkedIn\\n \\nGithub\\n \\nTelegram'),\n",
       " Document(metadata={'source': 'Yevheniia Ilchenko. Python_AI_ML.pdf', 'page': 0}, page_content='Github\\n \\nTelegram\\n \\nReady to work autonomously in case of blackout\\n \\nProfile'),\n",
       " Document(metadata={'source': 'Yevheniia Ilchenko. Python_AI_ML.pdf', 'page': 0}, page_content='Profile\\nPython engineer with analytical knowledge, strategic'),\n",
       " Document(metadata={'source': 'Yevheniia Ilchenko. Python_AI_ML.pdf', 'page': 0}, page_content='planning, and strong organizational skills, bringing with it a')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m----> 4\u001b[0m db \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mdocs\u001b[49m[:\u001b[38;5;241m5\u001b[39m], OpenAIEmbeddings())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "db = Chroma.from_documents(docs[:2], OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application development using Django/DRF/FastAPI.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are this library reserch paper ?\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs[:10], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Profile\\nPython engineer with analytical knowledge, strategic'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who are this reserch paper?\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000177D5605E10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000177D54D5EA0>, root_client=<openai.OpenAI object at 0x00000177D5605930>, root_async_client=<openai.AsyncOpenAI object at 0x00000177D5605DE0>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    " Answer the following question based on the provide context.\n",
    "think step by step before providing a detailed anwer. I will tip you $1000 if the user find the answer helpful.\n",
    "                            <context>\n",
    "                            {context}\n",
    "                            </context>\n",
    "                            Question:{input}\n",
    "                            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "doc_chain = create_stuff_documents_chain(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000177D394FB80>, search_kwargs={})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever_chain = create_retrieval_chain(retriever, doc_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retriever_chain.invoke({\"input\":\"describe about what this document\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, the document appears to be a profile or resume of a software engineer specializing in application development using Django/DRF/FastAPI. The individual has experience working with AI/ML technologies, specifically LLM and SD, and has basic skills in AWS/GCP. They are also able to work autonomously in case of blackout situations. Additionally, the individual is described as a Python engineer with analytical and strategic skills. Overall, the document seems to be a summary of the individual's technical skills, experience, and qualifications in the field of software engineering.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
